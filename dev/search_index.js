var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"api/#Continuous-Dynamic-Programming","page":"API Reference","title":"Continuous Dynamic Programming","text":"","category":"section"},{"location":"api/#ContinuousDPs.ContinuousDP","page":"API Reference","title":"ContinuousDPs.ContinuousDP","text":"ContinuousDP{N,TR,TS,Tf,Tg,Tlb,Tub}\n\nType that reperesents a continuous-state dynamic program\n\nFields\n\nf::Tf<:Function: Reward function\ng::Tg<:Function: State transition function\ndiscount::Float64: Discount factor\nshocks::TR<:AbstractVecOrMat: Random variables' nodes\nweights::Vector{Float64}: Random variables' weights\nx_lb::Tlb<:Function: Lower bound of action variables\nx_ub::Tub<:Function: Upper bound of action variables\ninterp::Interp{N,TS<:VecOrMat}: Object that contains information about interpolation\n\n\n\n\n\n","category":"type"},{"location":"api/#LQ-Approximation","page":"API Reference","title":"LQ Approximation","text":"","category":"section"},{"location":"api/#ContinuousDPs.approx_lq","page":"API Reference","title":"ContinuousDPs.approx_lq","text":"approx_lq(s_star, x_star, f_star, Df_star, DDf_star, g_star, Dg_star,\n          discount)\n\nReturn an approximating LQ instance.\n\nArguments\n\ns_star::ScalarOrArray{T}: State variables at the steady-state\nx_star::ScalarOrArray{T}: Action variables at the steady-state\nf_star::Real: Reward function evaluated at the steady-state\nDf_star::AbstractVector{T}: Gradient of f satisfying Df_star = [f_s', f_x']\nDDf_star::AbstractMatrix{T}: Hessian of f satisfying DDf_star::Array = [f_ss f_sx; f_xs f_xx]\ng_star::ScalarOrArray{T}: State transition function evaluated at the steady-state\nDg_star::AbstractMatrix{T}: Jacobian of g satisfying Dg_star = [g_s, g_x]\ndiscount::Real: Discount factor\n\n\n\n\n\n","category":"function"},{"location":"api/#Simulation","page":"API Reference","title":"Simulation","text":"","category":"section"},{"location":"api/#QuantEcon.simulate","page":"API Reference","title":"QuantEcon.simulate","text":"simulate([rng=GLOBAL_RNG], res, s_init, ts_length)\n\nGenerate a sample path of state variable(s)\n\nArguments\n\nrng::AbstractRNG: Random number generator\nres::CDPSolveResult: Object that contains result of dynamic programming\ns_init: Initial value of state variable(s)\nts_length::Integer: Length of simulation\n\nReturn\n\ns_path::VecOrMat:: Generated sample path of state variable(s)\n\n\n\n\n\n","category":"function"},{"location":"api/#Policy-Evaluation","page":"API Reference","title":"Policy Evaluation","text":"","category":"section"},{"location":"api/#ContinuousDPs.evaluate_policy!","page":"API Reference","title":"ContinuousDPs.evaluate_policy!","text":"evaluate_policy!(cdp, X, C)\n\nUpdate basis coefficients\n\nArguments\n\ncdp::ContinuousDP: Object that contains model parameters\nX::Vector{Float64}: Policy function vector\nC::Vector{Float64}: A buffer array to hold the basis coefficients\n\nReturns\n\nC::Vector{Float64}: Updated basis coefficients vector\n\n\n\n\n\n","category":"function"},{"location":"api/#ContinuousDPs.set_eval_nodes!","page":"API Reference","title":"ContinuousDPs.set_eval_nodes!","text":"set_eval_nodes!(res, s_nodes_coord)\n\nSet evaluation nodes\n\nArguments\n\nres::CDPSolveResult: Object that contains the result of dynamic programming\ns_nodes_coord::NTuple{N,AbstractVector}: Evaluation nodes\n\n\n\n\n\n","category":"function"},{"location":"#ContinuousDPs.jl","page":"Home","title":"ContinuousDPs.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Build Status) (Image: codecov) (Image: Documentation)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Routines for solving continuous state dynamic programs by the Bellman equation collocation method.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the package, open the Julia package manager (Pkg) and type:","category":"page"},{"location":"","page":"Home","title":"Home","text":"add https://github.com/QuantEcon/ContinuousDPs.jl","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using ContinuousDPs\n\n# Create a continuous dynamic programming problem\n# (example code here would depend on the specific API)","category":"page"},{"location":"#Demo-Notebooks","page":"Home","title":"Demo Notebooks","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Stochastic optimal growth model\nExamples from Miranda and Fackler 2002, Chapter 9","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"M. J. Miranda and P. L. Fackler, Applied Computational Economics and Finance, MIT press, 2002.","category":"page"}]
}
